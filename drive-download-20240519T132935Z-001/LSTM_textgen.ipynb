{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOIFwJVpsZsEA9TUDfQTy2u"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qhmwICDCVz7j","executionInfo":{"status":"ok","timestamp":1712663156234,"user_tz":-330,"elapsed":14224,"user":{"displayName":"Sudharshanan S","userId":"06602243793536797192"}},"outputId":"231358f8-c6d7-40b7-b4a5-189f6f07f406"},"outputs":[{"output_type":"stream","name":"stdout","text":["[[7, 8, 9, 10, 11, 12]]\n","[[13, 14, 15, 16, 2, 17, 18]]\n","[[3, 19, 20, 21, 22, 23, 24, 25]]\n","[[4, 26, 27, 2, 28, 4, 29]]\n","[[3, 30, 31, 5, 32, 33, 34, 35]]\n","[[36, 1, 37, 38, 39, 6, 40, 41]]\n","[[42, 43, 44, 45, 46, 47]]\n","[[48, 1, 49, 5, 1, 50, 6, 51, 52]]\n","Epoch 1/100\n","2/2 [==============================] - 5s 24ms/step - loss: 3.9727 - accuracy: 0.0196\n","Epoch 2/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.9645 - accuracy: 0.0588\n","Epoch 3/100\n","2/2 [==============================] - 0s 30ms/step - loss: 3.9571 - accuracy: 0.0784\n","Epoch 4/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.9499 - accuracy: 0.0784\n","Epoch 5/100\n","2/2 [==============================] - 0s 26ms/step - loss: 3.9402 - accuracy: 0.0784\n","Epoch 6/100\n","2/2 [==============================] - 0s 23ms/step - loss: 3.9265 - accuracy: 0.0784\n","Epoch 7/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.9057 - accuracy: 0.0784\n","Epoch 8/100\n","2/2 [==============================] - 0s 25ms/step - loss: 3.8775 - accuracy: 0.0784\n","Epoch 9/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.8374 - accuracy: 0.0784\n","Epoch 10/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.7903 - accuracy: 0.0784\n","Epoch 11/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.7723 - accuracy: 0.0784\n","Epoch 12/100\n","2/2 [==============================] - 0s 23ms/step - loss: 3.7423 - accuracy: 0.0784\n","Epoch 13/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.7018 - accuracy: 0.0784\n","Epoch 14/100\n","2/2 [==============================] - 0s 25ms/step - loss: 3.6573 - accuracy: 0.0784\n","Epoch 15/100\n","2/2 [==============================] - 0s 23ms/step - loss: 3.6216 - accuracy: 0.0784\n","Epoch 16/100\n","2/2 [==============================] - 0s 25ms/step - loss: 3.5757 - accuracy: 0.0784\n","Epoch 17/100\n","2/2 [==============================] - 0s 25ms/step - loss: 3.5178 - accuracy: 0.0980\n","Epoch 18/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.4570 - accuracy: 0.0980\n","Epoch 19/100\n","2/2 [==============================] - 0s 25ms/step - loss: 3.3883 - accuracy: 0.0980\n","Epoch 20/100\n","2/2 [==============================] - 0s 23ms/step - loss: 3.3164 - accuracy: 0.1176\n","Epoch 21/100\n","2/2 [==============================] - 0s 28ms/step - loss: 3.2136 - accuracy: 0.1569\n","Epoch 22/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.1307 - accuracy: 0.1569\n","Epoch 23/100\n","2/2 [==============================] - 0s 24ms/step - loss: 3.0356 - accuracy: 0.1569\n","Epoch 24/100\n","2/2 [==============================] - 0s 28ms/step - loss: 2.9590 - accuracy: 0.1373\n","Epoch 25/100\n","2/2 [==============================] - 0s 29ms/step - loss: 2.8994 - accuracy: 0.1765\n","Epoch 26/100\n","2/2 [==============================] - 0s 23ms/step - loss: 2.8056 - accuracy: 0.1569\n","Epoch 27/100\n","2/2 [==============================] - 0s 25ms/step - loss: 2.7362 - accuracy: 0.2157\n","Epoch 28/100\n","2/2 [==============================] - 0s 23ms/step - loss: 2.6485 - accuracy: 0.2353\n","Epoch 29/100\n","2/2 [==============================] - 0s 23ms/step - loss: 2.5365 - accuracy: 0.2745\n","Epoch 30/100\n","2/2 [==============================] - 0s 24ms/step - loss: 2.4519 - accuracy: 0.2745\n","Epoch 31/100\n","2/2 [==============================] - 0s 24ms/step - loss: 2.4144 - accuracy: 0.3137\n","Epoch 32/100\n","2/2 [==============================] - 0s 23ms/step - loss: 2.3407 - accuracy: 0.3333\n","Epoch 33/100\n","2/2 [==============================] - 0s 23ms/step - loss: 2.2879 - accuracy: 0.3725\n","Epoch 34/100\n","2/2 [==============================] - 0s 33ms/step - loss: 2.1843 - accuracy: 0.3725\n","Epoch 35/100\n","2/2 [==============================] - 0s 25ms/step - loss: 2.1284 - accuracy: 0.4314\n","Epoch 36/100\n","2/2 [==============================] - 0s 26ms/step - loss: 2.0742 - accuracy: 0.4902\n","Epoch 37/100\n","2/2 [==============================] - 0s 30ms/step - loss: 2.0228 - accuracy: 0.5294\n","Epoch 38/100\n","2/2 [==============================] - 0s 24ms/step - loss: 1.9585 - accuracy: 0.5098\n","Epoch 39/100\n","2/2 [==============================] - 0s 23ms/step - loss: 1.8566 - accuracy: 0.6078\n","Epoch 40/100\n","2/2 [==============================] - 0s 28ms/step - loss: 1.8384 - accuracy: 0.6275\n","Epoch 41/100\n","2/2 [==============================] - 0s 40ms/step - loss: 1.7525 - accuracy: 0.6667\n","Epoch 42/100\n","2/2 [==============================] - 0s 23ms/step - loss: 1.7611 - accuracy: 0.6275\n","Epoch 43/100\n","2/2 [==============================] - 0s 23ms/step - loss: 1.6704 - accuracy: 0.6275\n","Epoch 44/100\n","2/2 [==============================] - 0s 39ms/step - loss: 1.6199 - accuracy: 0.6667\n","Epoch 45/100\n","2/2 [==============================] - 0s 33ms/step - loss: 1.5844 - accuracy: 0.6667\n","Epoch 46/100\n","2/2 [==============================] - 0s 27ms/step - loss: 1.5304 - accuracy: 0.6863\n","Epoch 47/100\n","2/2 [==============================] - 0s 34ms/step - loss: 1.5314 - accuracy: 0.6863\n","Epoch 48/100\n","2/2 [==============================] - 0s 24ms/step - loss: 1.4486 - accuracy: 0.6667\n","Epoch 49/100\n","2/2 [==============================] - 0s 23ms/step - loss: 1.4273 - accuracy: 0.7059\n","Epoch 50/100\n","2/2 [==============================] - 0s 30ms/step - loss: 1.4001 - accuracy: 0.6471\n","Epoch 51/100\n","2/2 [==============================] - 0s 30ms/step - loss: 1.3896 - accuracy: 0.7059\n","Epoch 52/100\n","2/2 [==============================] - 0s 28ms/step - loss: 1.3481 - accuracy: 0.6471\n","Epoch 53/100\n","2/2 [==============================] - 0s 25ms/step - loss: 1.3435 - accuracy: 0.7451\n","Epoch 54/100\n","2/2 [==============================] - 0s 27ms/step - loss: 1.3022 - accuracy: 0.6863\n","Epoch 55/100\n","2/2 [==============================] - 0s 30ms/step - loss: 1.2853 - accuracy: 0.7059\n","Epoch 56/100\n","2/2 [==============================] - 0s 39ms/step - loss: 1.3043 - accuracy: 0.7255\n","Epoch 57/100\n","2/2 [==============================] - 0s 38ms/step - loss: 1.2368 - accuracy: 0.7059\n","Epoch 58/100\n","2/2 [==============================] - 0s 28ms/step - loss: 1.1929 - accuracy: 0.7059\n","Epoch 59/100\n","2/2 [==============================] - 0s 24ms/step - loss: 1.2150 - accuracy: 0.7451\n","Epoch 60/100\n","2/2 [==============================] - 0s 37ms/step - loss: 1.1609 - accuracy: 0.7059\n","Epoch 61/100\n","2/2 [==============================] - 0s 23ms/step - loss: 1.1160 - accuracy: 0.7647\n","Epoch 62/100\n","2/2 [==============================] - 0s 25ms/step - loss: 1.1161 - accuracy: 0.7843\n","Epoch 63/100\n","2/2 [==============================] - 0s 23ms/step - loss: 1.0939 - accuracy: 0.7647\n","Epoch 64/100\n","2/2 [==============================] - 0s 24ms/step - loss: 1.0624 - accuracy: 0.7843\n","Epoch 65/100\n","2/2 [==============================] - 0s 25ms/step - loss: 1.0538 - accuracy: 0.7843\n","Epoch 66/100\n","2/2 [==============================] - 0s 26ms/step - loss: 1.0193 - accuracy: 0.8235\n","Epoch 67/100\n","2/2 [==============================] - 0s 23ms/step - loss: 1.0132 - accuracy: 0.7843\n","Epoch 68/100\n","2/2 [==============================] - 0s 26ms/step - loss: 0.9947 - accuracy: 0.8235\n","Epoch 69/100\n","2/2 [==============================] - 0s 25ms/step - loss: 0.9792 - accuracy: 0.8039\n","Epoch 70/100\n","2/2 [==============================] - 0s 26ms/step - loss: 0.9541 - accuracy: 0.8039\n","Epoch 71/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.9522 - accuracy: 0.7647\n","Epoch 72/100\n","2/2 [==============================] - 0s 26ms/step - loss: 0.9386 - accuracy: 0.8039\n","Epoch 73/100\n","2/2 [==============================] - 0s 27ms/step - loss: 0.9139 - accuracy: 0.8039\n","Epoch 74/100\n","2/2 [==============================] - 0s 33ms/step - loss: 0.9120 - accuracy: 0.7647\n","Epoch 75/100\n","2/2 [==============================] - 0s 27ms/step - loss: 0.8930 - accuracy: 0.8431\n","Epoch 76/100\n","2/2 [==============================] - 0s 32ms/step - loss: 0.8815 - accuracy: 0.8431\n","Epoch 77/100\n","2/2 [==============================] - 0s 27ms/step - loss: 0.8581 - accuracy: 0.8824\n","Epoch 78/100\n","2/2 [==============================] - 0s 28ms/step - loss: 0.8475 - accuracy: 0.8431\n","Epoch 79/100\n","2/2 [==============================] - 0s 23ms/step - loss: 0.8378 - accuracy: 0.8627\n","Epoch 80/100\n","2/2 [==============================] - 0s 22ms/step - loss: 0.8199 - accuracy: 0.8824\n","Epoch 81/100\n","2/2 [==============================] - 0s 23ms/step - loss: 0.8088 - accuracy: 0.8627\n","Epoch 82/100\n","2/2 [==============================] - 0s 23ms/step - loss: 0.8034 - accuracy: 0.8627\n","Epoch 83/100\n","2/2 [==============================] - 0s 23ms/step - loss: 0.7870 - accuracy: 0.8824\n","Epoch 84/100\n","2/2 [==============================] - 0s 30ms/step - loss: 0.7767 - accuracy: 0.8824\n","Epoch 85/100\n","2/2 [==============================] - 0s 26ms/step - loss: 0.7682 - accuracy: 0.9020\n","Epoch 86/100\n","2/2 [==============================] - 0s 23ms/step - loss: 0.7493 - accuracy: 0.9020\n","Epoch 87/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.7454 - accuracy: 0.9020\n","Epoch 88/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.7353 - accuracy: 0.9020\n","Epoch 89/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.7242 - accuracy: 0.9020\n","Epoch 90/100\n","2/2 [==============================] - 0s 23ms/step - loss: 0.7175 - accuracy: 0.8824\n","Epoch 91/100\n","2/2 [==============================] - 0s 23ms/step - loss: 0.7071 - accuracy: 0.8824\n","Epoch 92/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.7033 - accuracy: 0.8627\n","Epoch 93/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.6906 - accuracy: 0.8824\n","Epoch 94/100\n","2/2 [==============================] - 0s 27ms/step - loss: 0.6970 - accuracy: 0.8824\n","Epoch 95/100\n","2/2 [==============================] - 0s 26ms/step - loss: 0.6733 - accuracy: 0.8824\n","Epoch 96/100\n","2/2 [==============================] - 0s 25ms/step - loss: 0.6683 - accuracy: 0.9020\n","Epoch 97/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.6542 - accuracy: 0.9216\n","Epoch 98/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.6467 - accuracy: 0.9216\n","Epoch 99/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.6425 - accuracy: 0.8824\n","Epoch 100/100\n","2/2 [==============================] - 0s 24ms/step - loss: 0.6267 - accuracy: 0.9020\n","From fairest creatures we desire increase substantial fuel fuel fuel fuel fuel decease decease decease\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","# Sample corpus (Shakespeare's Sonnet 1)\n","corpus = [\n","    \"From fairest creatures we desire increase,\",\n","    \"That thereby beauty's rose might never die,\",\n","    \"But as the riper should by time decease,\",\n","    \"His tender heir might bear his memory:\",\n","    \"But thou, contracted to thine own bright eyes,\",\n","    \"Feed'st thy light's flame with self-substantial fuel,\",\n","    \"Making a famine where abundance lies,\",\n","    \"Thyself thy foe, to thy sweet self too cruel.\",\n","]\n","\n","# Tokenization\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# Create input sequences\n","input_sequences = []\n","for line in corpus:\n","    token_list = tokenizer.texts_to_sequences([line])[0]\n","    print(tokenizer.texts_to_sequences([line]))\n","    for i in range(1, len(token_list)):\n","        n_gram_sequence = token_list[:i+1]\n","        input_sequences.append(n_gram_sequence)\n","\n","# Padding sequences\n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre')\n","\n","# Create predictors and label\n","predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n","\n","# Convert label to one-hot encoding\n","label = tf.keras.utils.to_categorical(label, num_classes=total_words)\n","\n","# Build LSTM Model\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(total_words, 100, input_length=max_sequence_len-1),\n","    tf.keras.layers.LSTM(150, return_sequences=True),\n","    tf.keras.layers.LSTM(100),\n","    tf.keras.layers.Dense(total_words, activation='softmax')\n","])\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(predictors, label, epochs=100, verbose=1)\n","\n","# Generate text\n","def generate_text(seed_text, next_words, max_sequence_len):\n","    for _ in range(next_words):\n","        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n","        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","        predicted_probs = model.predict(token_list, verbose=0)\n","        predicted_index = np.argmax(predicted_probs)\n","        output_word = \"\"\n","        for word, index in tokenizer.word_index.items():\n","            if index == predicted_index:\n","                output_word = word\n","                break\n","        seed_text += \" \" + output_word\n","    return seed_text\n","\n","# Sample input\n","input_text = \"From fairest creatures we desire\"\n","\n","# Generate text\n","generated_text = generate_text(input_text,10, max_sequence_len)\n","print(generated_text)\n"]},{"cell_type":"code","source":[],"metadata":{"id":"PNuXy7zPrUoX"},"execution_count":null,"outputs":[]}]}